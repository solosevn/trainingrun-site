<!DOCTYPE html>
<html lang="en">
<head>
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-20ZZ8BG99X"></script>
        <script>
                  window.dataLayer = window.dataLayer || [];
                  function gtag(){dataLayer.push(arguments);}
                  gtag('js', new Date());
                  gtag('config', 'G-20ZZ8BG99X');
        </script>
        </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TRS Methodology | Training Run</title>
    <meta name="description" content="The complete methodology behind the Training Run Score (TRS) - how we measure and rank AI model performance with full source citations.">
    <meta property="og:title" content="TRS Methodology | Training Run">
    <meta property="og:description" content="Transparent, fact-checked methodology for scoring AI models. Full source citations.">
    <link rel="canonical" href="https://trainingrun.ai/trsmethodology">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&family=Space+Grotesk:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">

    <style>
        /* ===== TOC Sidebar ===== */
        .toc-sidebar {
            position: fixed;
            left: 20px;
            top: 150px;
            width: 200px;
            background: rgba(10, 15, 26, 0.95);
            border: 1px solid rgba(0, 229, 255, 0.15);
            border-radius: 12px;
            padding: 16px;
            z-index: 100;
            backdrop-filter: blur(10px);
            transition: opacity 0.3s;
        }
        .toc-header {
            font-size: 11px;
            text-transform: uppercase;
            letter-spacing: 1.5px;
            color: #00e5ff;
            margin-bottom: 12px;
            padding-bottom: 8px;
            border-bottom: 1px solid rgba(0, 229, 255, 0.2);
        }
        .toc-list {
            list-style: none;
            padding: 0;
            margin: 0;
        }
        .toc-list li { margin-bottom: 4px; }
        .toc-divider {
            height: 1px;
            background: rgba(255,255,255,0.08);
            margin: 8px 0;
        }
        .toc-link {
            display: block;
            padding: 4px 8px;
            font-size: 12px;
            color: rgba(255,255,255,0.5);
            text-decoration: none;
            border-radius: 4px;
            transition: all 0.2s;
            border-left: 2px solid transparent;
        }
        .toc-link:hover {
            color: #fff;
            background: rgba(0, 229, 255, 0.08);
        }
        .toc-link.active {
            color: #00e5ff;
            border-left-color: #00e5ff;
            background: rgba(0, 229, 255, 0.05);
        }
        .toc-weight {
            display: inline-block;
            width: 30px;
            font-size: 10px;
            font-weight: 700;
            color: #00e5ff;
            opacity: 0.8;
        }

        /* ===== Summary Table ===== */
        .summary-table-wrapper {
            overflow-x: auto;
            margin: 20px 0;
        }
        .summary-table {
            width: 100%;
            border-collapse: collapse;
            font-size: 14px;
        }
        .summary-table thead th {
            background: rgba(0, 229, 255, 0.1);
            color: #00e5ff;
            padding: 10px 14px;
            text-align: left;
            font-size: 11px;
            text-transform: uppercase;
            letter-spacing: 1px;
            border-bottom: 1px solid rgba(0, 229, 255, 0.3);
        }
        .summary-table tbody td {
            padding: 10px 14px;
            border-bottom: 1px solid rgba(255,255,255,0.06);
            color: rgba(255,255,255,0.75);
        }
        .summary-table tbody tr:hover {
            background: rgba(0, 229, 255, 0.03);
        }
        .summary-table a {
            color: #00e5ff;
            text-decoration: none;
        }
        .summary-table a:hover { text-decoration: underline; }
        .weight-cell {
            font-weight: 700;
            text-align: center;
            border-radius: 4px;
            padding: 4px 8px;
        }
        .weight-high { color: #00e5ff; background: rgba(0, 229, 255, 0.12); }
        .weight-med { color: #a78bfa; background: rgba(167, 139, 250, 0.12); }
        .weight-low { color: rgba(255,255,255,0.5); background: rgba(255,255,255,0.05); }

        /* ===== Accordion Sections ===== */
        .method-section h2 {
            cursor: pointer;
            position: relative;
            padding-right: 40px;
            user-select: none;
        }
        .method-section h2::after {
            content: '';
            position: absolute;
            right: 10px;
            top: 50%;
            transform: translateY(-50%);
            width: 10px;
            height: 10px;
            border-right: 2px solid rgba(0, 229, 255, 0.5);
            border-bottom: 2px solid rgba(0, 229, 255, 0.5);
            transform: translateY(-50%) rotate(45deg);
            transition: transform 0.3s;
        }
        .method-section.collapsed h2::after {
            transform: translateY(-30%) rotate(-45deg);
        }
        .method-section .section-content {
            overflow: hidden;
            transition: max-height 0.4s ease, opacity 0.3s ease;
            max-height: 5000px;
            opacity: 1;
        }
        .method-section.collapsed .section-content {
            max-height: 0;
            opacity: 0;
        }
        /* Don't collapse exec summary or quick ref */
        #section-exec-summary h2::after,
        #section-quick-ref h2::after {
            display: none;
        }
        #section-exec-summary h2,
        #section-quick-ref h2 {
            cursor: default;
        }

        /* ===== Adjust main content for sidebar ===== */
        .methodology-page {
            margin-left: 240px;
        }

        /* ===== Responsive: hide sidebar on small screens ===== */
        @media (max-width: 1100px) {
            .toc-sidebar { display: none; }
            .methodology-page { margin-left: 0; }
        }
    </style>
</head>
<body>
    <div class="bg-animation"></div>
    <nav>
        <a href="/" class="logo">Training Run</a>
        <ul class="nav-links">
            <li><a href="/trsmethodology" class="active">TRS Methodology</a></li>
            <li><a href="/scores">Current Scores</a></li>
            <li><a href="/about">About</a></li>
        </ul>
    </nav>
    <a href="#" onclick="history.back(); return false;" class="back-btn"><svg viewBox="0 0 24 24"><path d="M19 12H5M12 19l-7-7 7-7"/></svg> Back</a>
    <article class="methodology-page">
    <!-- Sticky TOC Sidebar -->
    <aside class="toc-sidebar" id="toc-sidebar">
        <div class="toc-header">On This Page</div>
        <ul class="toc-list">
            <li><a href="#section-exec-summary" class="toc-link active">Executive Summary</a></li>
            <li class="toc-divider"></li>
            <li><a href="#section-safety" class="toc-link"><span class="toc-weight">21%</span> Safety</a></li>
            <li><a href="#section-reasoning" class="toc-link"><span class="toc-weight">20%</span> Reasoning</a></li>
            <li><a href="#section-coding" class="toc-link"><span class="toc-weight">20%</span> Coding</a></li>
            <li><a href="#section-human-pref" class="toc-link"><span class="toc-weight">18%</span> Human Preference</a></li>
            <li><a href="#section-knowledge" class="toc-link"><span class="toc-weight">8%</span> Knowledge</a></li>
            <li><a href="#section-efficiency" class="toc-link"><span class="toc-weight">7%</span> Efficiency</a></li>
            <li><a href="#section-usage" class="toc-link"><span class="toc-weight">6%</span> Usage/Adoption</a></li>
            <li class="toc-divider"></li>
            <li><a href="#section-limitations" class="toc-link">Limitations</a></li>
            <li><a href="#section-citations" class="toc-link">Citations</a></li>
        </ul>
    </aside>
        <header class="page-header">
            <h1>TRS <span class="cyan">Methodology</span></h1>
            <p class="page-intro">The Training Run Score (TRS) is a composite metric designed to provide a clear, comparable measure of AI model capabilities. This page documents our complete methodology with full source citations.</p>
            <p class="last-updated">Last Updated: February 11, 2026</p>
        </header>
        <section class="method-section" id="section-exec-summary">
            <h2>Executive Summary</h2>
            <div class="summary-box">
                <p>The TRS aggregates performance data from established benchmarks into a single 0-100 score. We weight seven dimensions based on their relevance to real-world AI utility:</p>
                <div class="formula-display">
                    <code>TRS = (S x 0.21) + (R x 0.20) + (C x 0.20) + (H x 0.18) + (K x 0.08) + (E x 0.07) + (U x 0.06)</code>
                </div>
                <p class="formula-note">Where S = Safety, R = Reasoning, C = Coding, H = Human Preference, K = Knowledge, E = Efficiency, U = Usage Adoption</p>
            </div>
        </section>
    <section class="method-section" id="section-quick-ref">
        <h2>Quick Reference</h2>
        <div class="summary-table-wrapper">
            <table class="summary-table">
                <thead>
                    <tr>
                        <th>Dimension</th>
                        <th>Weight</th>
                        <th>Primary Source(s)</th>
                        <th>What It Measures</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><a href="#section-safety">Safety &amp; Reliability</a></td>
                        <td class="weight-cell weight-high">21%</td>
                        <td>HELM Safety, TrustLLM, MLCommons, Enkrypt AI</td>
                        <td>Harm avoidance, fairness, misuse prevention, governance</td>
                    </tr>
                    <tr>
                        <td><a href="#section-reasoning">Reasoning &amp; Logic</a></td>
                        <td class="weight-cell weight-high">20%</td>
                        <td>ARC-AGI-2, GPQA Diamond, MATH</td>
                        <td>Novel problem-solving, multi-step reasoning</td>
                    </tr>
                    <tr>
                        <td><a href="#section-coding">Coding Proficiency</a></td>
                        <td class="weight-cell weight-high">20%</td>
                        <td>SWE-Bench, LiveCodeBench, SciCode</td>
                        <td>Real-world code generation and debugging</td>
                    </tr>
                    <tr>
                        <td><a href="#section-human-pref">Human Preference</a></td>
                        <td class="weight-cell weight-med">18%</td>
                        <td>LMSYS Chatbot Arena Elo</td>
                        <td>Blind human preference in side-by-side comparisons</td>
                    </tr>
                    <tr>
                        <td><a href="#section-knowledge">Knowledge</a></td>
                        <td class="weight-cell weight-low">8%</td>
                        <td>MMLU-Pro, TruthfulQA</td>
                        <td>Factual accuracy, comprehension breadth</td>
                    </tr>
                    <tr>
                        <td><a href="#section-efficiency">Efficiency</a></td>
                        <td class="weight-cell weight-low">7%</td>
                        <td>Artificial Analysis</td>
                        <td>Tokens/second output speed</td>
                    </tr>
                    <tr>
                        <td><a href="#section-usage">Usage/Adoption</a></td>
                        <td class="weight-cell weight-low">6%</td>
                        <td>OpenRouter Rankings</td>
                        <td>Real-world developer usage share</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </section>
        <section class="method-section" id="section-reasoning">
            <h2>1. Reasoning and Logic (20%)</h2>
            <div class="section-content">
            <div class="method-content">
                <h3>What We Measure</h3>
                <p>The ability to solve novel problems requiring multi-step reasoning, logical deduction, and genuine understanding.</p>
                <h3>Primary Data Sources</h3>
                <div class="source-citation">
                    <div class="source-item">
                        <h4>ARC-AGI-2 (ARC Prize Foundation)</h4>
                        <p>The Abstraction and Reasoning Corpus tests novel reasoning on tasks specifically designed to require thinking through new problems.</p>
                        <ul class="source-details">
                            <li><strong>Current best baseline:</strong> 31% accuracy</li>
                            <li><strong>With refinement loops:</strong> 54% accuracy</li>
                            <li><strong>Human average:</strong> 60% accuracy</li>
                        </ul>
                        <a href="https://arcprize.org/" target="_blank" rel="noopener" class="source-link">Source: arcprize.org</a>
                    </div>
                    <div class="source-item">
                        <h4>GPQA Diamond (NYU, Anthropic, et al.)</h4>
                        <p>Graduate-level science questions written by PhD experts.</p>
                        <a href="https://arxiv.org/abs/2311.12022" target="_blank" rel="noopener" class="source-link">Source: arXiv:2311.12022</a>
                    </div>
                    <div class="source-item">
                        <h4>MATH (Hendrycks et al.)</h4>
                        <p>Competition-level mathematics problems from AMC, AIME, and Olympiad competitions.</p>
                        <a href="https://arxiv.org/abs/2103.03874" target="_blank" rel="noopener" class="source-link">Source: arXiv:2103.03874</a>
                    </div>
                </div>
                <h3>Score Calculation</h3>
                <code class="calc-formula">R = (ARC x 0.40) + (GPQA x 0.35) + (MATH x 0.25)</code>
            </div>
        
            </div></section>
        <section class="method-section" id="section-coding">
            <h2>2. Coding Proficiency (20%)</h2>
            <div class="section-content">
            <div class="method-content">
                <h3>What We Measure</h3>
                <p>Can this model actually write code that works? Real software engineering tasks that mirror what professional developers face daily.</p>
                <h3>Why This Matters for TRS</h3>
                <p>Coding ability is one of the clearest demonstrations of an AI model's practical intelligence. A model that can debug complex codebases, implement features across multiple files, and write maintainable code provides genuine utility.</p>
                <h3>Primary Data Sources</h3>
                <div class="source-citation">
                    <div class="source-item">
                        <h4>SWE-Bench Verified (50% of Coding Score)</h4>
                        <p>Percentage of real GitHub issues resolved correctly. These are actual bugs and feature requests from production repositories.</p>
                        <a href="https://www.swebench.com/" target="_blank" rel="noopener" class="source-link">Source: swebench.com</a>
                    </div>
                    <div class="source-item">
                        <h4>LiveCodeBench (25% of Coding Score)</h4>
                        <p>Performance on coding problems released after model training cutoffs. Eliminates benchmark contamination.</p>
                        <a href="https://livecodebench.github.io/" target="_blank" rel="noopener" class="source-link">Source: livecodebench.github.io</a>
                    </div>
                    <div class="source-item">
                        <h4>SciCode (15% of Coding Score)</h4>
                        <p>Ability to solve scientific research coding challenges.</p>
                        <a href="https://scicode-bench.github.io/" target="_blank" rel="noopener" class="source-link">Source: scicode-bench.github.io</a>
                    </div>
                    <div class="source-item">
                        <h4>Legacy Benchmarks (10% of Coding Score)</h4>
                        <p>HumanEval and MBPP measure basic coding competency on isolated function problems.</p>
                        <a href="https://arxiv.org/abs/2107.03374" target="_blank" rel="noopener" class="source-link">Source: arXiv:2107.03374 (HumanEval)</a>
                        <a href="https://arxiv.org/abs/2108.07732" target="_blank" rel="noopener" class="source-link">Source: arXiv:2108.07732 (MBPP)</a>
                    </div>
                </div>
                <h3>Score Calculation</h3>
                <code class="calc-formula">C = (C_SWE x 0.50) + (C_LCB x 0.25) + (C_SciCode x 0.15) + (C_Legacy x 0.10)</code>
                <p class="formula-note">Where: C_SWE = Normalized SWE-Bench Verified score, C_LCB = Normalized LiveCodeBench score, C_SciCode = Normalized SciCode score, C_Legacy = Average of normalized HumanEval and MBPP scores</p>
            </div>
        
            </div></section>
        <section class="method-section" id="section-human-pref">
            <h2>3. Human Preference (18%)</h2>
            <div class="section-content">
            <div class="method-content">
                <h3>What We Measure</h3>
                <p>When real people compare model outputs side-by-side, which model do they actually like more? This captures helpfulness, clarity, tone, and instruction-following.</p>
                <h3>Why We Use LMSYS Chatbot Arena</h3>
                <p>We use LMSYS Chatbot Arena as our primary signal because it is the most widely adopted, transparent, and actively maintained open platform for live LLM comparisons.</p>
                <ul class="source-details">
                    <li><strong>Blind, pairwise voting:</strong> Users chat with two anonymous models and vote for the better response.</li>
                    <li><strong>Large-scale:</strong> Hundreds of thousands to over a million human preference votes.</li>
                    <li><strong>Elo rating system:</strong> Similar to chess, maintains stable relative rankings.</li>
                    <li><strong>Model-agnostic:</strong> Both proprietary and open-weight models evaluated equally.</li>
                </ul>
                <h3>Known Limitations</h3>
                <ul class="source-details">
                    <li><strong>Prompt and user bias:</strong> Skewed toward English and tech-savvy users.</li>
                    <li><strong>Style vs. substance:</strong> Style can influence wins.</li>
                    <li><strong>Elo nuances:</strong> Small gaps may not be statistically meaningful.</li>
                    <li><strong>Partial observability:</strong> Only aggregate ratings are public.</li>
                </ul>
                <h3>Primary Data Source</h3>
                <div class="source-citation">
                    <div class="source-item">
                        <h4>LMSYS Chatbot Arena (UC Berkeley, LMSYS Group)</h4>
                        <p>Live, web-based evaluation where users interact with two anonymous models and vote for the better response.</p>
                        <a href="https://huggingface.co/spaces/lmarena-ai/chatbot-arena/" target="_blank" rel="noopener" class="source-link">Source: huggingface.co/spaces/lmarena-ai/chatbot-arena</a>
                        <a href="https://arxiv.org/abs/2403.04132" target="_blank" rel="noopener" class="source-link">Methodology: arXiv:2403.04132</a>
                    </div>
                </div>
                <h3>Score Calculation</h3>
                <code class="calc-formula">H = ((Model_Elo - Min_Elo) / (Max_Elo - Min_Elo)) x 100</code>
                <p class="formula-note">Maps the lowest-rated model to 0, highest to 100. Human Preference contributes 18% of TRS.</p>
            </div>
        
            </div></section>
        <section class="method-section" id="section-knowledge">
            <h2>4. Knowledge and Comprehension (8%)</h2>
            <div class="section-content">
            <div class="method-content">
                <h3>What We Measure</h3>
                <p>Breadth and depth of factual knowledge across academic domains.</p>
                <h3>Primary Data Sources</h3>
                <div class="source-citation">
                    <div class="source-item">
                        <h4>MMLU (Hendrycks et al.)</h4>
                        <p>57 subjects from STEM to humanities, 14,000+ questions.</p>
                        <a href="https://arxiv.org/abs/2009.03300" target="_blank" rel="noopener" class="source-link">Source: arXiv:2009.03300</a>
                    </div>
                    <div class="source-item">
                        <h4>TruthfulQA (Lin et al.)</h4>
                        <p>Tests whether models generate truthful answers.</p>
                        <a href="https://arxiv.org/abs/2109.07958" target="_blank" rel="noopener" class="source-link">Source: arXiv:2109.07958</a>
                    </div>
                </div>
                <h3>Score Calculation</h3>
                <code class="calc-formula">K = (MMLU x 0.70) + (TruthfulQA x 0.30)</code>
            </div>
        
            </div></section>
        <section class="method-section" id="section-efficiency">
            <h2>5. Efficiency and Cost (7%)</h2>
            <div class="section-content">
            <div class="method-content">
                <h3>What We Measure</h3>
                <p>Performance per dollar.</p>
                <h3>Data Sources</h3>
                <div class="source-citation">
                    <div class="source-item">
                        <h4>API Pricing</h4>
                        <p>Official pricing from OpenAI, Anthropic, Google.</p>
                    </div>
                </div>
                <h3>Score Calculation</h3>
                <code class="calc-formula">E = (Average_Benchmark_Score / Cost_Per_1M_Tokens) x Normalization_Factor</code>
            </div>
        
            </div></section>
        <section class="method-section" id="section-safety">
            <h2>6. Safety and Reliability (21%)</h2>
            <div class="section-content">
            <div class="method-content">
                <h3>What We Measure</h3>
                <p>Harm avoidance, fairness, misuse prevention, adversarial robustness, and company-level safety governance. Safety is weighted highest because responsible AI ensures progress does not derail due to unchecked risks, while maintaining strong weights on Reasoning and Coding to drive technological advancement.</p>
                <h3>Why Safety Is Weighted #1</h3>
                <p>Multiple independent, quantifiable benchmarks now exist to measure AI safety objectively. By weighting safety highest, TRS incentivizes the development of models that are both capable and responsible, enabling high abundance for all humanity without stifling innovation.</p>
                <h3>Sub-Component Breakdown</h3>
                <div class="source-citation">
                    <div class="source-item">
                        <h4>Harm Avoidance (40% of Safety Score)</h4>
                        <p>Source: HELM Safety (Stanford CRFM). Unified framework aggregating 5 sub-benchmarks across 24+ models with 0-1 normalized scores.</p>
                        <a href="https://crfm.stanford.edu/helm/safety/latest/" target="_blank" rel="noopener" class="source-link">crfm.stanford.edu/helm/safety</a>
                    </div>
                    <div class="source-item">
                        <h4>Fairness (30% of Safety Score)</h4>
                        <p>Source: TrustLLM. Comprehensive trustworthiness evaluation covering fairness, privacy, robustness, and ethical alignment across major LLMs.</p>
                        <a href="https://trustllmbenchmark.github.io/TrustLLM-Website/" target="_blank" rel="noopener" class="source-link">trustllmbenchmark.github.io</a>
                    </div>
                    <div class="source-item">
                        <h4>Misuse Prevention (20% of Safety Score)</h4>
                        <p>Source: MLCommons AI Luminate. Industry consortium standard testing 12 hazard categories with 24,000+ prompts per language.</p>
                        <a href="https://ailuminate.mlcommons.org/benchmarks/" target="_blank" rel="noopener" class="source-link">ailuminate.mlcommons.org/benchmarks</a>
                    </div>
                    <div class="source-item">
                        <h4>Governance (10% of Safety Score)</h4>
                        <p>Sources: AI Safety Index (expert-reviewed company-level safety practices, 35 indicators, A-F grades) and Enkrypt AI Leaderboard (NIST RMF + OWASP Top 10 aligned, 200+ models, continuously updated).</p>
                        <a href="https://futureoflife.org/ai-safety-index-winter-2025/" target="_blank" rel="noopener" class="source-link">futureoflife.org/ai-safety-index</a>
                        <a href="https://leaderboard.enkryptai.com/" target="_blank" rel="noopener" class="source-link">leaderboard.enkryptai.com</a>
                    </div>
                </div>
                <h3>Score Calculation</h3>
                <p><code class="calc-formula">S = (HELM_Safety x 0.40) + (TrustLLM x 0.30) + (AILuminate x 0.20) + (Governance x 0.10)</code></p>
            </div>
        
            </div></section>
        <section class="method-section" id="section-usage">
            <h2>7. Usage Adoption (6%)</h2>
            <div class="section-content">
            <div class="method-content">
                <h3>What We Measure</h3>
                <p>Real-world adoption and developer usage patterns, measured by normalized token consumption share across models and providers.</p>
                <h3>Data Source</h3>
                <div class="source-citation">
                    <div class="source-item">
                        <h4>OpenRouter Rankings</h4>
                        <p>Token consumption data across 300+ models from 60+ providers. Normalized token share in key categories.</p>
                        <a href="https://openrouter.ai/rankings" target="_blank" rel="noopener" class="source-link">openrouter.ai/rankings</a>
                    </div>
                </div>
                <h3>Known Limitations</h3>
                <ul class="source-details">
                    <li><strong>Measures popularity, not quality</strong> - High usage does not equal high performance</li>
                    <li><strong>Platform selection bias</strong> - Reflects OpenRouter user base, not all AI usage</li>
                    <li><strong>Not independently auditable</strong> - Token data is self-reported by OpenRouter</li>
                </ul>
                <h3>Score Calculation</h3>
                <p><code class="calc-formula">U = (Model_Token_Share / Max_Token_Share) x 100</code></p>
            </div>
        
            </div></section>
        <section class="method-section" id="section-limitations">
            <h2>Important Limitations</h2>
            <div class="section-content">
            <div class="limitations-box">
                <h3>What TRS Does NOT Measure</h3>
                <ul>
                    <li><strong>Future capabilities:</strong> TRS measures current performance only</li>
                    <li><strong>AGI proximity:</strong> We make no claims about AGI</li>
                    <li><strong>Real-world deployment:</strong> Benchmark performance may differ from production</li>
                </ul>
                <h3>Known Biases</h3>
                <ul>
                    <li>English-language bias in most benchmarks</li>
                    <li>Potential training data contamination</li>
                    <li>Self-reported safety data from providers</li>
                </ul>
            </div>
        
            </div></section>
        <section class="method-section" id="section-update-freq">
            <h2>Update Frequency</h2>
            <div class="section-content">
            <p>TRS scores are updated <strong>weekly</strong>, typically on Mondays.</p>
        
            </div></section>
        <section class="method-section" id="section-citations">
            <h2>Full Citation List</h2>
            <div class="section-content">
            <div class="citations-list">
                <h3>Evaluation Platforms</h3>
                <ul>
                    <li>LMSYS Chatbot Arena: <a href="https://huggingface.co/spaces/lmarena-ai/chatbot-arena/" target="_blank" rel="noopener">huggingface.co/spaces/lmarena-ai/chatbot-arena</a></li>
                    <li>ARC Prize: <a href="https://arcprize.org/" target="_blank" rel="noopener">arcprize.org</a></li>
                    <li>SWE-Bench: <a href="https://www.swebench.com/" target="_blank" rel="noopener">swebench.com</a></li>
                    <li>LiveCodeBench: <a href="https://livecodebench.github.io/" target="_blank" rel="noopener">livecodebench.github.io</a></li>
                    <li>SciCode: <a href="https://scicode-bench.github.io/" target="_blank" rel="noopener">scicode-bench.github.io</a></li>
                </ul>
                <h3>Safety Benchmarks</h3>
                <ul>
                    <li>HELM Safety (Stanford CRFM): <a href="https://crfm.stanford.edu/helm/safety/latest/" target="_blank" rel="noopener">crfm.stanford.edu/helm/safety</a></li>
                    <li>TrustLLM: <a href="https://trustllmbenchmark.github.io/TrustLLM-Website/" target="_blank" rel="noopener">trustllmbenchmark.github.io</a></li>
                    <li>MLCommons AI Luminate: <a href="https://ailuminate.mlcommons.org/benchmarks/" target="_blank" rel="noopener">ailuminate.mlcommons.org/benchmarks</a></li>
                    <li>AI Safety Index (FLI): <a href="https://futureoflife.org/ai-safety-index-winter-2025/" target="_blank" rel="noopener">futureoflife.org/ai-safety-index</a></li>
                    <li>Enkrypt AI Leaderboard: <a href="https://leaderboard.enkryptai.com/" target="_blank" rel="noopener">leaderboard.enkryptai.com</a></li>
                </ul>
                <h3>Usage Adoption</h3>
                <ul>
                    <li>OpenRouter Rankings: <a href="https://openrouter.ai/rankings" target="_blank" rel="noopener">openrouter.ai/rankings</a></li>
                </ul>
                <h3>Efficiency and Pricing</h3>
                <ul>
                    <li>Artificial Analysis: <a href="https://artificialanalysis.ai/" target="_blank" rel="noopener">artificialanalysis.ai</a></li>
                </ul>
            </div>
        
            </div></section>
    </article>
    <footer>
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <a href="/" class="logo">Training Run</a>
                    <p>Your weekly AI conditioning.</p>
                </div>
                <div class="footer-links">
                    <h4>Resources</h4>
                    <a href="/trsmethodology">TRS Methodology</a>
                    <a href="/scores">Current Scores</a>
                    <a href="/about">About Us</a>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2026 Training Run. All rights reserved.</p>
            </div>
        </div>
    </footer>

    <script>
    document.addEventListener('DOMContentLoaded', function() {
        // Accordion toggle
        document.querySelectorAll('.method-section h2').forEach(function(h2) {
            var section = h2.closest('.method-section');
            var id = section ? section.id : '';
            // Don't make exec-summary or quick-ref collapsible
            if (id === 'section-exec-summary' || id === 'section-quick-ref') return;
            
            h2.addEventListener('click', function() {
                section.classList.toggle('collapsed');
            });
        });

        // Start dimension sections collapsed (except exec-summary, quick-ref)
        var collapseSections = ['section-reasoning','section-coding','section-human-pref',
            'section-knowledge','section-efficiency','section-safety','section-usage',
            'section-limitations','section-update-freq','section-citations'];
        collapseSections.forEach(function(id) {
            var el = document.getElementById(id);
            if (el) el.classList.add('collapsed');
        });

        // TOC scroll tracking
        var tocLinks = document.querySelectorAll('.toc-link');
        var sections = [];
        tocLinks.forEach(function(link) {
            var href = link.getAttribute('href');
            if (href && href.startsWith('#')) {
                var target = document.getElementById(href.substring(1));
                if (target) sections.push({ el: target, link: link });
            }
        });

        // Smooth scroll on TOC click
        tocLinks.forEach(function(link) {
            link.addEventListener('click', function(e) {
                e.preventDefault();
                var href = link.getAttribute('href');
                var target = document.getElementById(href.substring(1));
                if (target) {
                    // Expand if collapsed
                    if (target.classList.contains('collapsed')) {
                        target.classList.remove('collapsed');
                    }
                    target.scrollIntoView({ behavior: 'smooth', block: 'start' });
                }
            });
        });

        // Highlight active TOC link on scroll
        window.addEventListener('scroll', function() {
            var scrollPos = window.scrollY + 150;
            var active = null;
            sections.forEach(function(s) {
                if (s.el.offsetTop <= scrollPos) active = s;
            });
            tocLinks.forEach(function(l) { l.classList.remove('active'); });
            if (active) active.link.classList.add('active');
        });
    });
    </script>
</body>
</html>
