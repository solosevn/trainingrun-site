<!DOCTYPE html>
<html lang="en">
<head>
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-20ZZ8BG99X"></script>
        <script>
                  window.dataLayer = window.dataLayer || [];
                  function gtag(){dataLayer.push(arguments);}
                  gtag('js', new Date());
                  gtag('config', 'G-20ZZ8BG99X');
        </script>
        </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TRS Methodology | Training Run</title>
    <meta name="description" content="The complete methodology behind the Training Run Score (TRS) - how we measure and rank AI model performance with full source citations.">
    <meta property="og:title" content="TRS Methodology | Training Run">
    <meta property="og:description" content="Transparent, fact-checked methodology for scoring AI models. Full source citations.">
    <link rel="canonical" href="https://trainingrun.ai/trsmethodology">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&family=Space+Grotesk:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">

    <style>
        /* ===== TOC Sidebar ===== */
        .toc-sidebar {
            position: fixed;
            left: 20px;
            top: 150px;
            width: 200px;
            background: rgba(10, 15, 26, 0.95);
            border: 1px solid rgba(0, 229, 255, 0.15);
            border-radius: 12px;
            padding: 16px;
            z-index: 100;
            backdrop-filter: blur(10px);
            transition: opacity 0.3s;
        }
        .toc-header {
            font-size: 11px;
            text-transform: uppercase;
            letter-spacing: 1.5px;
            color: #00e5ff;
            margin-bottom: 12px;
            padding-bottom: 8px;
            border-bottom: 1px solid rgba(0, 229, 255, 0.2);
        }
        .toc-list {
            list-style: none;
            padding: 0;
            margin: 0;
        }
        .toc-list li { margin-bottom: 4px; }
        .toc-divider {
            height: 1px;
            background: rgba(255,255,255,0.08);
            margin: 8px 0;
        }
        .toc-link {
            display: block;
            padding: 4px 8px;
            font-size: 12px;
            color: rgba(255,255,255,0.5);
            text-decoration: none;
            border-radius: 4px;
            transition: all 0.2s;
            border-left: 2px solid transparent;
        }
        .toc-link:hover {
            color: #fff;
            background: rgba(0, 229, 255, 0.08);
        }
        .toc-link.active {
            color: #00e5ff;
            border-left-color: #00e5ff;
            background: rgba(0, 229, 255, 0.05);
        }
        .toc-weight {
            display: inline-block;
            width: 30px;
            font-size: 10px;
            font-weight: 700;
            color: #00e5ff;
            opacity: 0.8;
        }

        /* ===== Summary Table ===== */
        .summary-table-wrapper {
            overflow-x: auto;
            margin: 20px 0;
        }
        .summary-table {
            width: 100%;
            border-collapse: collapse;
            font-size: 14px;
        }
        .summary-table thead th {
            background: rgba(0, 229, 255, 0.1);
            color: #00e5ff;
            padding: 10px 14px;
            text-align: left;
            font-size: 11px;
            text-transform: uppercase;
            letter-spacing: 1px;
            border-bottom: 1px solid rgba(0, 229, 255, 0.3);
        }
        .summary-table tbody td {
            padding: 10px 14px;
            border-bottom: 1px solid rgba(255,255,255,0.06);
            color: rgba(255,255,255,0.75);
        }
        .summary-table tbody tr:hover {
            background: rgba(0, 229, 255, 0.03);
        }
        .summary-table a {
            color: #00e5ff;
            text-decoration: none;
        }
        .summary-table a:hover { text-decoration: underline; }
        .weight-cell {
            font-weight: 700;
            text-align: center;
            border-radius: 4px;
            padding: 4px 8px;
        }
        .weight-high { color: #00e5ff; background: rgba(0, 229, 255, 0.12); }
        .weight-med { color: #a78bfa; background: rgba(167, 139, 250, 0.12); }
        .weight-low { color: rgba(255,255,255,0.5); background: rgba(255,255,255,0.05); }

        /* ===== Accordion Sections ===== */
        .method-section h2 {
            cursor: pointer;
            position: relative;
            padding-right: 40px;
            user-select: none;
        }
        .method-section h2::after {
            content: '';
            position: absolute;
            right: 10px;
            top: 50%;
            transform: translateY(-50%);
            width: 10px;
            height: 10px;
            border-right: 2px solid rgba(0, 229, 255, 0.5);
            border-bottom: 2px solid rgba(0, 229, 255, 0.5);
            transform: translateY(-50%) rotate(45deg);
            transition: transform 0.3s;
        }
        .method-section.collapsed h2::after {
            transform: translateY(-30%) rotate(-45deg);
        }
        .method-section .section-content {
            overflow: hidden;
            transition: max-height 0.4s ease, opacity 0.3s ease;
            max-height: 5000px;
            opacity: 1;
        }
        .method-section.collapsed .section-content {
            max-height: 0;
            opacity: 0;
        }
        /* Don't collapse exec summary or quick ref */
        #section-exec-summary h2::after,
        #section-quick-ref h2::after {
            display: none;
        }
        #section-exec-summary h2,
        #section-quick-ref h2 {
            cursor: default;
        }

        /* ===== Adjust main content for sidebar ===== */
        .methodology-page {
            margin-left: 240px;
        }

        /* ===== Responsive: hide sidebar on small screens ===== */
        @media (max-width: 1100px) {
            .toc-sidebar { display: none; }
            .methodology-page { margin-left: 0; }
        }
    </style>
</head>
<body>
    <div class="bg-animation"></div>
    <nav>
        <a href="/" class="logo">Training Run</a>
        <ul class="nav-links">
            <li><a href="/trsmethodology" class="active">TRS Methodology</a></li>
            <li><a href="/trscode">TRScode</a></li>
            <li><a href="/scores">Current Scores</a></li>
            <li><a href="/about">About</a></li>
        </ul>
    </nav>
    <a href="#" onclick="history.back(); return false;" class="back-btn"><svg viewBox="0 0 24 24"><path d="M19 12H5M12 19l-7-7 7-7"/></svg> Back</a>
    <article class="methodology-page">
    <!-- Sticky TOC Sidebar -->
    <aside class="toc-sidebar" id="toc-sidebar">
        <div class="toc-header">On This Page</div>
        <ul class="toc-list">
            <li><a href="#section-exec-summary" class="toc-link active">Executive Summary</a></li>
            <li class="toc-divider"></li>
            <li><a href="#section-safety" class="toc-link"><span class="toc-weight">21%</span> Safety</a></li>
            <li><a href="#section-reasoning" class="toc-link"><span class="toc-weight">20%</span> Reasoning</a></li>
            <li><a href="#section-coding" class="toc-link"><span class="toc-weight">20%</span> Coding</a></li>
            <li><a href="#section-human-pref" class="toc-link"><span class="toc-weight">18%</span> Human Preference</a></li>
            <li><a href="#section-knowledge" class="toc-link"><span class="toc-weight">8%</span> Knowledge</a></li>
            <li><a href="#section-efficiency" class="toc-link"><span class="toc-weight">7%</span> Efficiency</a></li>
            <li><a href="#section-usage" class="toc-link"><span class="toc-weight">6%</span> Usage/Adoption</a></li>
            <li class="toc-divider"></li>
            <li><a href="#section-limitations" class="toc-link">Limitations</a></li>
            <li><a href="#section-citations" class="toc-link">Citations</a></li>
        </ul>
    </aside>
        <header class="page-header">
            <h1>TRS <span class="cyan">Methodology</span></h1>
            <p class="page-intro">The Training Run Score (TRS) is a composite metric designed to provide a clear, comparable measure of AI model capabilities. This page documents our complete methodology with full source citations.</p>
            <p class="last-updated">Last Updated: February 22, 2026</p>
        </header>
        <section class="method-section" id="section-exec-summary">
            <h2>Executive Summary</h2>
            <div class="summary-box">
                <p>The TRS aggregates performance data from established benchmarks into a single 0-100 score. We weight seven dimensions based on their relevance to real-world AI utility:</p>
                <div class="formula-display">
                    <code>TRS = (S x 0.21) + (R x 0.20) + (C x 0.20) + (H x 0.18) + (K x 0.08) + (E x 0.07) + (U x 0.06)</code>
                </div>
                <p class="formula-note">Where S = Safety, R = Reasoning, C = Coding, H = Human Preference, K = Knowledge, E = Efficiency, U = Usage Adoption</p>
            </div>
        </section>
    <section class="method-section" id="section-quick-ref">
        <h2>Quick Reference</h2>
        <div class="summary-table-wrapper">
            <table class="summary-table">
                <thead>
                    <tr>
                        <th>Dimension</th>
                        <th>Weight</th>
                        <th>Primary Source(s)</th>
                        <th>What It Measures</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><a href="#section-safety">Safety &amp; Reliability</a></td>
                        <td class="weight-cell weight-high">21%</td>
                        <td>HELM Safety, AIR-Bench</td>
                        <td>Harm avoidance, fairness, misuse prevention, governance</td>
                    </tr>
                    <tr>
                        <td><a href="#section-reasoning">Reasoning &amp; Logic</a></td>
                        <td class="weight-cell weight-high">20%</td>
                        <td>ARC-AGI-2, LiveBench Reasoning, HELM Capabilities</td>
                        <td>Novel problem-solving, multi-step reasoning</td>
                    </tr>
                    <tr>
                        <td><a href="#section-coding">Coding Proficiency</a></td>
                        <td class="weight-cell weight-high">20%</td>
                        <td>SWE-bench Verified, EvalPlus, LiveCodeBench, SWE-rebench</td>
                        <td>Real-world code generation and debugging</td>
                    </tr>
                    <tr>
                        <td><a href="#section-human-pref">Human Preference</a></td>
                        <td class="weight-cell weight-med">18%</td>
                        <td>Arena (arena.ai), AlpacaEval</td>
                        <td>Blind human preference in side-by-side comparisons</td>
                    </tr>
                    <tr>
                        <td><a href="#section-knowledge">Knowledge</a></td>
                        <td class="weight-cell weight-low">8%</td>
                        <td>MMLU-Pro, HELM MMLU, SimpleQA</td>
                        <td>Factual accuracy, comprehension breadth</td>
                    </tr>
                    <tr>
                        <td><a href="#section-efficiency">Efficiency</a></td>
                        <td class="weight-cell weight-low">7%</td>
                        <td>Artificial Analysis, PricePerToken</td>
                        <td>Tokens/second output speed</td>
                    </tr>
                    <tr>
                        <td><a href="#section-usage">Usage/Adoption</a></td>
                        <td class="weight-cell weight-low">6%</td>
                        <td>OpenRouter Rankings</td>
                        <td>Real-world developer usage share</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </section>
        <section class="method-section" id="section-reasoning">
            <h2>1. Reasoning and Logic (20%)</h2>
            <div class="section-content">
            <div class="method-content">
                <h3>What We Measure</h3>
                <p>The ability to solve novel problems requiring multi-step reasoning, logical deduction, and genuine understanding.</p>
                <h3>Primary Data Sources</h3>
                <div class="source-citation">
                    <div class="source-item">
                        <h4>ARC-AGI-2 (ARC Prize Foundation)</h4>
                        <p>The Abstraction and Reasoning Corpus tests novel reasoning on tasks specifically designed to require thinking through new problems.</p>
                        <ul class="source-details">
                            <li><strong>Current best baseline:</strong> 31% accuracy</li>
                            <li><strong>With refinement loops:</strong> 54% accuracy</li>
                            <li><strong>Human average:</strong> 60% accuracy</li>
                        </ul>
                        <a href="https://arcprize.org/" target="_blank" rel="noopener" class="source-link">Source: arcprize.org</a>
                    </div>
                    <div class="source-item">
                        <h4>LiveBench Reasoning (LiveBench.ai)</h4>
                        <p>Contamination-free reasoning tasks updated monthly with new questions drawn from recent competitions and papers. Tests math, logic, and multi-step deduction.</p>
                        <a href="https://livebench.ai/" target="_blank" rel="noopener" class="source-link">Source: livebench.ai</a>
                    </div>
                    <div class="source-item">
                        <h4>HELM Capabilities (Stanford CRFM)</h4>
                        <p>Stanford's holistic evaluation of reasoning, commonsense, and world knowledge across a broad suite of tasks — including math, science, and language understanding.</p>
                        <a href="https://crfm.stanford.edu/helm/capabilities/latest/" target="_blank" rel="noopener" class="source-link">Source: crfm.stanford.edu/helm/capabilities</a>
                    </div>
                </div>
                <h3>Score Calculation</h3>
                <code class="calc-formula">R = normalize_sources_and_merge(ARC-AGI-2, LiveBench_Reasoning, HELM_Capabilities)</code>
                <p class="formula-note">Each source normalized 0–100. Composite contributes 20% of TRS.</p>
            </div>
        
            </div></section>
        <section class="method-section" id="section-coding">
          <h2>2. Coding Proficiency (20%)</h2>
          <div class="section-content">
            <div class="method-content">
              <h3>What We Measure</h3>
              <p>Can this model actually write code that works? Real software engineering tasks that mirror what professional developers face daily — from fixing production bugs to autonomous terminal navigation.</p>

              <div class="source-item" style="background: linear-gradient(135deg, rgba(0,212,255,0.06), rgba(0,212,255,0.02)); border-color: rgba(0,212,255,0.25); margin: 20px 0;">
                <h4 style="color: #00d4ff;">TRScode — Standalone Coding Intelligence Score</h4>
                <p>TRScode is Training Run’s dedicated coding composite — a weighted score built from 8 sub-metrics across 7 independent platforms. It provides a deeper, coding-only view beyond the 20% coding weight in the overall TRS. TRScode uses a 5-pillar structure: Real-World Issue Resolution (30%), Code Generation (25%), Agentic Coding (20%), Scientific &amp; Specialized (15%), and Human Preference (10%).</p>
                <a href="/trscode" class="source-link" style="color: #00d4ff;">Explore TRScode Scores →</a>
              </div>

              <h3>Why This Matters for TRS</h3>
              <p>Coding ability is one of the clearest demonstrations of an AI model’s practical intelligence. A model that can debug complex codebases, implement features across multiple files, navigate terminals autonomously, and write maintainable code provides genuine utility.</p>

              <h3>Primary Data Sources</h3>
              <div class="source-citation">
                <div class="source-item">
                  <h4>SWE-Bench Verified (17% of TRScode)</h4>
                  <p>Percentage of real GitHub issues resolved correctly. These are actual bugs and feature requests from production repositories, verified by human annotators.</p>
                  <a href="https://www.swebench.com/" target="_blank" rel="noopener" class="source-link">Source: swebench.com</a>
                </div>
                <div class="source-item">
                  <h4>SWE-rebench (13% of TRScode)</h4>
                  <p>A contamination-resistant fork of SWE-bench with fresh issues drawn after training cutoffs, ensuring genuine evaluation of issue resolution ability.</p>
                  <a href="https://swe-rebench.com/leaderboard" target="_blank" rel="noopener" class="source-link">Source: swe-rebench.com</a>
                </div>
                <div class="source-item">
                  <h4>LiveCodeBench (15% of TRScode)</h4>
                  <p>Performance on coding problems released after model training cutoffs. Eliminates benchmark contamination through continuously updated competitive programming challenges.</p>
                  <a href="https://livecodebench.github.io/" target="_blank" rel="noopener" class="source-link">Source: livecodebench.github.io</a>
                </div>
                <div class="source-item">
                  <h4>EvalPlus (25% of TRScode)</h4>
                  <p>Rigorous function-level code generation benchmark augmenting HumanEval and MBPP with 80× more test cases to catch edge-case failures missed by standard evaluation.</p>
                  <a href="https://evalplus.github.io/leaderboard.html" target="_blank" rel="noopener" class="source-link">Source: evalplus.github.io/leaderboard</a>
                </div>
              </div>

              <h3>Score Calculation</h3>
              <code class="calc-formula">TRScode = SWE-bench_Verified (25%) + SWE-rebench (20%) + LiveCodeBench (20%) + EvalPlus (20%) + Arena_Code_ELO (15%)</code>
              <p class="formula-note">Each model is scored 0–100 on each sub-metric (top performer = 100, others proportional). Models must have non-null scores in at least 2 of 5 pillars. Missing pillars contribute zero (Option B). The TRS Coding dimension draws from the TRScode composite.</p>
            </div>
        
            </div></section>
        <section class="method-section" id="section-human-pref">
            <h2>3. Human Preference (18%)</h2>
            <div class="section-content">
            <div class="method-content">
                <h3>What We Measure</h3>
                <p>When real people compare model outputs side-by-side, which model do they actually like more? This captures helpfulness, clarity, tone, and instruction-following.</p>
                <h3>Why We Use Arena (arena.ai)</h3>
                <p>We use Arena as our primary signal because it is the most widely adopted, transparent, and actively maintained open platform for live LLM comparisons. Arena (formerly LMSYS Chatbot Arena) is now hosted at arena.ai.</p>
                <ul class="source-details">
                    <li><strong>Blind, pairwise voting:</strong> Users chat with two anonymous models and vote for the better response.</li>
                    <li><strong>Large-scale:</strong> Hundreds of thousands to over a million human preference votes.</li>
                    <li><strong>Elo rating system:</strong> Similar to chess, maintains stable relative rankings.</li>
                    <li><strong>Model-agnostic:</strong> Both proprietary and open-weight models evaluated equally.</li>
                </ul>
                <h3>Known Limitations</h3>
                <ul class="source-details">
                    <li><strong>Prompt and user bias:</strong> Skewed toward English and tech-savvy users.</li>
                    <li><strong>Style vs. substance:</strong> Style can influence wins.</li>
                    <li><strong>Elo nuances:</strong> Small gaps may not be statistically meaningful.</li>
                    <li><strong>Partial observability:</strong> Only aggregate ratings are public.</li>
                </ul>
                <h3>Primary Data Source</h3>
                <div class="source-citation">
                    <div class="source-item">
                        <h4>Arena Overall (arena.ai)</h4>
                        <p>Live, web-based evaluation where users interact with two anonymous models and vote for the better response. Full leaderboard with 200+ models across all task types.</p>
                        <a href="https://arena.ai/leaderboard" target="_blank" rel="noopener" class="source-link">Source: arena.ai/leaderboard</a>
                    </div>
                    <div class="source-item">
                        <h4>Arena Text ELO (arena.ai)</h4>
                        <p>Text-specific ELO sub-leaderboard from Arena, isolating human preference on open-ended text and instruction-following tasks.</p>
                        <a href="https://arena.ai/leaderboard" target="_blank" rel="noopener" class="source-link">Source: arena.ai/leaderboard</a>
                    </div>
                    <div class="source-item">
                        <h4>AlpacaEval 2.0 (Stanford, et al.)</h4>
                        <p>Automated win-rate benchmark against GPT-4 using 805 instructions. Length-controlled win rate eliminates verbosity bias.</p>
                        <a href="https://tatsu-lab.github.io/alpaca_eval/" target="_blank" rel="noopener" class="source-link">Source: tatsu-lab.github.io/alpaca_eval</a>
                    </div>
                </div>
                <h3>Score Calculation</h3>
                <code class="calc-formula">H = normalize_sources_and_merge(Arena_Overall, Arena_Text, AlpacaEval)</code>
                <p class="formula-note">Each source normalized 0–100. Composite contributes 18% of TRS.</p>
            </div>
        
            </div></section>
        <section class="method-section" id="section-knowledge">
            <h2>4. Knowledge and Comprehension (8%)</h2>
            <div class="section-content">
            <div class="method-content">
                <h3>What We Measure</h3>
                <p>Breadth and depth of factual knowledge across academic domains.</p>
                <h3>Primary Data Sources</h3>
                <div class="source-citation">
                    <div class="source-item">
                        <h4>MMLU-Pro (Carnegie Mellon, et al.)</h4>
                        <p>A harder, contamination-resistant version of MMLU with 12,000+ expert-level questions across 14 disciplines. Uses 10-choice format to reduce guessing.</p>
                        <a href="https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro" target="_blank" rel="noopener" class="source-link">Source: TIGER-Lab/MMLU-Pro</a>
                    </div>
                    <div class="source-item">
                        <h4>HELM MMLU (Stanford CRFM)</h4>
                        <p>Stanford's standardized MMLU evaluation within the HELM framework, ensuring consistent prompting and scoring methodology across models.</p>
                        <a href="https://crfm.stanford.edu/helm/capabilities/latest/" target="_blank" rel="noopener" class="source-link">Source: crfm.stanford.edu/helm/capabilities</a>
                    </div>
                    <div class="source-item">
                        <h4>SimpleQA (OpenAI)</h4>
                        <p>Short, factual questions with unambiguous ground-truth answers. Tests whether models can reliably state facts without hallucinating.</p>
                        <a href="https://openai.com/index/introducing-simpleqa/" target="_blank" rel="noopener" class="source-link">Source: openai.com/simpleqa</a>
                    </div>
                </div>
                <h3>Score Calculation</h3>
                <code class="calc-formula">K = normalize_sources_and_merge(MMLU-Pro, HELM_MMLU, SimpleQA)</code>
                <p class="formula-note">Each source normalized 0–100. Composite contributes 8% of TRS.</p>
            </div>
        
            </div></section>
        <section class="method-section" id="section-efficiency">
            <h2>5. Efficiency and Cost (7%)</h2>
            <div class="section-content">
            <div class="method-content">
                <h3>What We Measure</h3>
                <p>Performance per dollar.</p>
                <h3>Data Sources</h3>
                <div class="source-citation">
                    <div class="source-item">
                        <h4>Artificial Analysis</h4>
                        <p>Independent benchmarking of model speed (tokens/second), latency, and pricing across API providers. Updated continuously with live measurements.</p>
                        <a href="https://artificialanalysis.ai/" target="_blank" rel="noopener" class="source-link">Source: artificialanalysis.ai</a>
                    </div>
                    <div class="source-item">
                        <h4>PricePerToken.com</h4>
                        <p>Crowd-sourced, continuously updated database of LLM API pricing across all major providers — input, output, and context window costs.</p>
                        <a href="https://pricepertoken.com/" target="_blank" rel="noopener" class="source-link">Source: pricepertoken.com</a>
                    </div>
                </div>
                <h3>Score Calculation</h3>
                <code class="calc-formula">E = normalize_sources_and_merge(Artificial_Analysis, PricePerToken)</code>
                <p class="formula-note">Higher score = better performance per dollar. Contributes 7% of TRS.</p>
            </div>
        
            </div></section>
        <section class="method-section" id="section-safety">
            <h2>6. Safety and Reliability (21%)</h2>
            <div class="section-content">
            <div class="method-content">
                <h3>What We Measure</h3>
                <p>Harm avoidance, fairness, misuse prevention, adversarial robustness, and company-level safety governance. Safety is weighted highest because responsible AI ensures progress does not derail due to unchecked risks, while maintaining strong weights on Reasoning and Coding to drive technological advancement.</p>
                <h3>Why Safety Is Weighted #1</h3>
                <p>Multiple independent, quantifiable benchmarks now exist to measure AI safety objectively. By weighting safety highest, TRS incentivizes the development of models that are both capable and responsible, enabling high abundance for all humanity without stifling innovation.</p>
                <h3>Primary Data Sources</h3>
                <div class="source-citation">
                    <div class="source-item">
                        <h4>HELM Safety (Stanford CRFM)</h4>
                        <p>Unified framework aggregating multiple safety sub-benchmarks. Tests harm avoidance, fairness, and adversarial robustness across 24+ models with 0–1 normalized scores.</p>
                        <a href="https://crfm.stanford.edu/helm/safety/latest/" target="_blank" rel="noopener" class="source-link">Source: crfm.stanford.edu/helm/safety</a>
                    </div>
                    <div class="source-item">
                        <h4>AIR-Bench (Berkeley, Stanford, et al.)</h4>
                        <p>Automated, iterative red-teaming benchmark covering 8 harm categories aligned with AI regulations (EU AI Act, US EO, China CAC). Continuously updated to avoid contamination.</p>
                        <a href="https://crfm.stanford.edu/helm/air-bench/v1.1.0/" target="_blank" rel="noopener" class="source-link">Source: crfm.stanford.edu/helm/air-bench</a>
                    </div>
                </div>
                <h3>Score Calculation</h3>
                <p><code class="calc-formula">S = normalize_sources_and_merge(HELM_Safety, AIR-Bench)</code></p>
                <p class="formula-note">Each source normalized 0–100. Composite contributes 21% of TRS.</p>
            </div>
        
            </div></section>
        <section class="method-section" id="section-usage">
            <h2>7. Usage Adoption (6%)</h2>
            <div class="section-content">
            <div class="method-content">
                <h3>What We Measure</h3>
                <p>Real-world adoption and developer usage patterns, measured by normalized token consumption share across models and providers.</p>
                <h3>Data Source</h3>
                <div class="source-citation">
                    <div class="source-item">
                        <h4>OpenRouter Rankings</h4>
                        <p>Token consumption data across 300+ models from 60+ providers. Normalized token share in key categories.</p>
                        <a href="https://openrouter.ai/rankings" target="_blank" rel="noopener" class="source-link">openrouter.ai/rankings</a>
                    </div>
                </div>
                <h3>Known Limitations</h3>
                <ul class="source-details">
                    <li><strong>Measures popularity, not quality</strong> - High usage does not equal high performance</li>
                    <li><strong>Platform selection bias</strong> - Reflects OpenRouter user base, not all AI usage</li>
                    <li><strong>Not independently auditable</strong> - Token data is self-reported by OpenRouter</li>
                </ul>
                <h3>Score Calculation</h3>
                <p><code class="calc-formula">U = (Model_Token_Share / Max_Token_Share) x 100</code></p>
            </div>
        
            </div></section>
        <section class="method-section" id="section-limitations">
            <h2>Important Limitations</h2>
            <div class="section-content">
            <div class="limitations-box">
                <h3>What TRS Does NOT Measure</h3>
                <ul>
                    <li><strong>Future capabilities:</strong> TRS measures current performance only</li>
                    <li><strong>AGI proximity:</strong> We make no claims about AGI</li>
                    <li><strong>Real-world deployment:</strong> Benchmark performance may differ from production</li>
                </ul>
                <h3>Known Biases</h3>
                <ul>
                    <li>English-language bias in most benchmarks</li>
                    <li>Potential training data contamination</li>
                    <li>Self-reported safety data from providers</li>
                </ul>
            </div>
        
            </div></section>
        <section class="method-section" id="section-update-freq">
            <h2>Update Frequency</h2>
            <div class="section-content">
            <p>TRS scores are updated <strong>weekly</strong>, typically on Mondays.</p>
        
            </div></section>
        <section class="method-section" id="section-citations">
            <h2>Full Citation List</h2>
            <div class="section-content">
            <div class="citations-list">
                <h3>Safety</h3>
                <ul>
                    <li>HELM Safety (Stanford CRFM): <a href="https://crfm.stanford.edu/helm/safety/latest/" target="_blank" rel="noopener">crfm.stanford.edu/helm/safety</a></li>
                    <li>AIR-Bench: <a href="https://crfm.stanford.edu/helm/air-bench/v1.1.0/" target="_blank" rel="noopener">crfm.stanford.edu/helm/air-bench</a></li>
                </ul>
                <h3>Reasoning</h3>
                <ul>
                    <li>ARC-AGI-2 (ARC Prize Foundation): <a href="https://arcprize.org/" target="_blank" rel="noopener">arcprize.org</a></li>
                    <li>LiveBench Reasoning: <a href="https://livebench.ai/" target="_blank" rel="noopener">livebench.ai</a></li>
                    <li>HELM Capabilities (Stanford CRFM): <a href="https://crfm.stanford.edu/helm/capabilities/latest/" target="_blank" rel="noopener">crfm.stanford.edu/helm/capabilities</a></li>
                </ul>
                <h3>Coding</h3>
                <ul>
                    <li>SWE-bench Verified: <a href="https://www.swebench.com/" target="_blank" rel="noopener">swebench.com</a></li>
                    <li>SWE-rebench: <a href="https://swe-rebench.com/leaderboard" target="_blank" rel="noopener">swe-rebench.com</a></li>
                    <li>LiveCodeBench: <a href="https://livecodebench.github.io/" target="_blank" rel="noopener">livecodebench.github.io</a></li>
                    <li>EvalPlus: <a href="https://evalplus.github.io/leaderboard.html" target="_blank" rel="noopener">evalplus.github.io/leaderboard</a></li>
                </ul>
                <h3>Human Preference</h3>
                <ul>
                    <li>Arena (arena.ai): <a href="https://arena.ai/leaderboard" target="_blank" rel="noopener">arena.ai/leaderboard</a></li>
                    <li>AlpacaEval 2.0: <a href="https://tatsu-lab.github.io/alpaca_eval/" target="_blank" rel="noopener">tatsu-lab.github.io/alpaca_eval</a></li>
                </ul>
                <h3>Knowledge</h3>
                <ul>
                    <li>MMLU-Pro: <a href="https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro" target="_blank" rel="noopener">huggingface.co/datasets/TIGER-Lab/MMLU-Pro</a></li>
                    <li>HELM MMLU (Stanford CRFM): <a href="https://crfm.stanford.edu/helm/capabilities/latest/" target="_blank" rel="noopener">crfm.stanford.edu/helm/capabilities</a></li>
                    <li>SimpleQA (OpenAI): <a href="https://openai.com/index/introducing-simpleqa/" target="_blank" rel="noopener">openai.com/simpleqa</a></li>
                </ul>
                <h3>Efficiency</h3>
                <ul>
                    <li>Artificial Analysis: <a href="https://artificialanalysis.ai/" target="_blank" rel="noopener">artificialanalysis.ai</a></li>
                    <li>PricePerToken: <a href="https://pricepertoken.com/" target="_blank" rel="noopener">pricepertoken.com</a></li>
                </ul>
                <h3>Usage &amp; Adoption</h3>
                <ul>
                    <li>OpenRouter Rankings: <a href="https://openrouter.ai/rankings" target="_blank" rel="noopener">openrouter.ai/rankings</a></li>
                </ul>
            </div>
        
            </div></section>
    </article>
    <footer>
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <a href="/" class="logo">Training Run</a>
                    <p>Your weekly AI conditioning.</p>
                </div>
                <div class="footer-links">
                    <h4>Resources</h4>
                    <a href="/trsmethodology">TRS Methodology</a>
                    <a href="/scores">Current Scores</a>
                    <a href="/about">About Us</a>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2026 Training Run. All rights reserved.</p>
            </div>
        </div>
    </footer>

    <script>
    document.addEventListener('DOMContentLoaded', function() {
        // Accordion toggle
        document.querySelectorAll('.method-section h2').forEach(function(h2) {
            var section = h2.closest('.method-section');
            var id = section ? section.id : '';
            // Don't make exec-summary or quick-ref collapsible
            if (id === 'section-exec-summary' || id === 'section-quick-ref') return;
            
            h2.addEventListener('click', function() {
                section.classList.toggle('collapsed');
            });
        });

        // Start dimension sections collapsed (except exec-summary, quick-ref)
        var collapseSections = ['section-reasoning','section-coding','section-human-pref',
            'section-knowledge','section-efficiency','section-safety','section-usage',
            'section-limitations','section-update-freq','section-citations'];
        collapseSections.forEach(function(id) {
            var el = document.getElementById(id);
            if (el) el.classList.add('collapsed');
        });

        // TOC scroll tracking
        var tocLinks = document.querySelectorAll('.toc-link');
        var sections = [];
        tocLinks.forEach(function(link) {
            var href = link.getAttribute('href');
            if (href && href.startsWith('#')) {
                var target = document.getElementById(href.substring(1));
                if (target) sections.push({ el: target, link: link });
            }
        });

        // Smooth scroll on TOC click
        tocLinks.forEach(function(link) {
            link.addEventListener('click', function(e) {
                e.preventDefault();
                var href = link.getAttribute('href');
                var target = document.getElementById(href.substring(1));
                if (target) {
                    // Expand if collapsed
                    if (target.classList.contains('collapsed')) {
                        target.classList.remove('collapsed');
                    }
                    target.scrollIntoView({ behavior: 'smooth', block: 'start' });
                }
            });
        });

        // Highlight active TOC link on scroll
        window.addEventListener('scroll', function() {
            var scrollPos = window.scrollY + 150;
            var active = null;
            sections.forEach(function(s) {
                if (s.el.offsetTop <= scrollPos) active = s;
            });
            tocLinks.forEach(function(l) { l.classList.remove('active'); });
            if (active) active.link.classList.add('active');
        });
    });
    </script>
</body>
</html>
