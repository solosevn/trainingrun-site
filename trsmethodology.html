<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TRS Methodology | Training Run</title>
    <meta name="description" content="The complete methodology behind the Training Run Score (TRS) - how we measure and rank AI model performance with full source citations.">
    <meta property="og:title" content="TRS Methodology | Training Run">
    <meta property="og:description" content="Transparent, fact-checked methodology for scoring AI models. Full source citations.">
    <link rel="canonical" href="https://trainingrun.ai/trsmethodology">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&family=Space+Grotesk:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="bg-animation"></div>
    <nav>
        <a href="/" class="logo">Training Run</a>
        <ul class="nav-links">
            <li><a href="/trsmethodology" class="active">TRS Methodology</a></li>
            <li><a href="/scores">Current Scores</a></li>
            <li><a href="/about">About</a></li>
        </ul>
    </nav>
    <article class="methodology-page">
        <header class="page-header">
            <h1>TRS <span class="cyan">Methodology</span></h1>
            <p class="page-intro">The Training Run Score (TRS) is a composite metric designed to provide a clear, comparable measure of AI model capabilities. This page documents our complete methodology with full source citations.</p>
            <p class="last-updated">Last Updated: February 2, 2026</p>
        </header>
        <section class="method-section">
            <h2>Executive Summary</h2>
            <div class="summary-box">
                <p>The TRS aggregates performance data from established benchmarks into a single 0-100 score. We weight six dimensions based on their relevance to real-world AI utility:</p>
                <div class="formula-display">
                    <code>TRS = (R x 0.25) + (C x 0.25) + (H x 0.20) + (K x 0.15) + (E x 0.10) + (S x 0.05)</code>
                </div>
                <p class="formula-note">Where R = Reasoning, C = Coding, H = Human Preference, K = Knowledge, E = Efficiency, S = Safety</p>
            </div>
        </section>
        <section class="method-section">
            <h2>1. Reasoning and Logic (25%)</h2>
            <div class="method-content">
                <h3>What We Measure</h3>
                <p>The ability to solve novel problems requiring multi-step reasoning, logical deduction, and genuine understanding.</p>
                <h3>Primary Data Sources</h3>
                <div class="source-citation">
                    <div class="source-item">
                        <h4>ARC-AGI-2 (ARC Prize Foundation)</h4>
                        <p>The Abstraction and Reasoning Corpus tests novel reasoning on tasks specifically designed to require thinking through new problems.</p>
                        <ul class="source-details">
                            <li><strong>Current best baseline:</strong> 31% accuracy</li>
                            <li><strong>With refinement loops:</strong> 54% accuracy</li>
                            <li><strong>Human average:</strong> 60% accuracy</li>
                        </ul>
                        <a href="https://arcprize.org/" target="_blank" rel="noopener" class="source-link">Source: arcprize.org</a>
                    </div>
                    <div class="source-item">
                        <h4>GPQA Diamond (NYU, Anthropic, et al.)</h4>
                        <p>Graduate-level science questions written by PhD experts.</p>
                        <a href="https://arxiv.org/abs/2311.12022" target="_blank" rel="noopener" class="source-link">Source: arXiv:2311.12022</a>
                    </div>
                    <div class="source-item">
                        <h4>MATH (Hendrycks et al.)</h4>
                        <p>Competition-level mathematics problems from AMC, AIME, and Olympiad competitions.</p>
                        <a href="https://arxiv.org/abs/2103.03874" target="_blank" rel="noopener" class="source-link">Source: arXiv:2103.03874</a>
                    </div>
                </div>
                <h3>Score Calculation</h3>
                <code class="calc-formula">R = (ARC x 0.40) + (GPQA x 0.35) + (MATH x 0.25)</code>
            </div>
        </section>
        <section class="method-section">
            <h2>2. Coding Proficiency (25%)</h2>
            <div class="method-content">
                <h3>What We Measure</h3>
                <p>Can this model actually write code that works? Real software engineering tasks that mirror what professional developers face daily.</p>
                <h3>Why This Matters for TRS</h3>
                <p>Coding ability is one of the clearest demonstrations of an AI model's practical intelligence. A model that can debug complex codebases, implement features across multiple files, and write maintainable code provides genuine utility.</p>
                <h3>Primary Data Sources</h3>
                <div class="source-citation">
                    <div class="source-item">
                        <h4>SWE-Bench Verified (50% of Coding Score)</h4>
                        <p>Percentage of real GitHub issues resolved correctly. These are actual bugs and feature requests from production repositories.</p>
                        <a href="https://www.swebench.com/" target="_blank" rel="noopener" class="source-link">Source: swebench.com</a>
                    </div>
                    <div class="source-item">
                        <h4>LiveCodeBench (25% of Coding Score)</h4>
                        <p>Performance on coding problems released after model training cutoffs. Eliminates benchmark contamination.</p>
                        <a href="https://livecodebench.github.io/" target="_blank" rel="noopener" class="source-link">Source: livecodebench.github.io</a>
                    </div>
                    <div class="source-item">
                        <h4>SciCode (15% of Coding Score)</h4>
                        <p>Ability to solve scientific research coding challenges.</p>
                        <a href="https://scicode-bench.github.io/" target="_blank" rel="noopener" class="source-link">Source: scicode-bench.github.io</a>
                    </div>
                    <div class="source-item">
                        <h4>Legacy Benchmarks (10% of Coding Score)</h4>
                        <p>HumanEval and MBPP measure basic coding competency on isolated function problems.</p>
                        <a href="https://arxiv.org/abs/2107.03374" target="_blank" rel="noopener" class="source-link">Source: arXiv:2107.03374 (HumanEval)</a>
                        <a href="https://arxiv.org/abs/2108.07732" target="_blank" rel="noopener" class="source-link">Source: arXiv:2108.07732 (MBPP)</a>
                    </div>
                </div>
                <h3>Score Calculation</h3>
                <code class="calc-formula">C = (C_SWE x 0.50) + (C_LCB x 0.25) + (C_SciCode x 0.15) + (C_Legacy x 0.10)</code>
                <p class="formula-note">Where: C_SWE = Normalized SWE-Bench Verified score, C_LCB = Normalized LiveCodeBench score, C_SciCode = Normalized SciCode score, C_Legacy = Average of normalized HumanEval and MBPP scores</p>
            </div>
        </section>
        <section class="method-section">
            <h2>3. Human Preference (20%)</h2>
            <div class="method-content">
                <h3>What We Measure</h3>
                <p>When real people compare model outputs side-by-side, which model do they actually like more? This captures helpfulness, clarity, tone, and instruction-following.</p>
                <h3>Why We Use LMSYS Chatbot Arena</h3>
                <p>We use LMSYS Chatbot Arena as our primary signal because it is the most widely adopted, transparent, and actively maintained open platform for live LLM comparisons.</p>
                <ul class="source-details">
                    <li><strong>Blind, pairwise voting:</strong> Users chat with two anonymous models and vote for the better response.</li>
                    <li><strong>Large-scale:</strong> Hundreds of thousands to over a million human preference votes.</li>
                    <li><strong>Elo rating system:</strong> Similar to chess, maintains stable relative rankings.</li>
                    <li><strong>Model-agnostic:</strong> Both proprietary and open-weight models evaluated equally.</li>
                </ul>
                <h3>Known Limitations</h3>
                <ul class="source-details">
                    <li><strong>Prompt and user bias:</strong> Skewed toward English and tech-savvy users.</li>
                    <li><strong>Style vs. substance:</strong> Style can influence wins.</li>
                    <li><strong>Elo nuances:</strong> Small gaps may not be statistically meaningful.</li>
                    <li><strong>Partial observability:</strong> Only aggregate ratings are public.</li>
                </ul>
                <h3>Primary Data Source</h3>
                <div class="source-citation">
                    <div class="source-item">
                        <h4>LMSYS Chatbot Arena (UC Berkeley, LMSYS Group)</h4>
                        <p>Live, web-based evaluation where users interact with two anonymous models and vote for the better response.</p>
                        <a href="https://huggingface.co/spaces/lmarena-ai/chatbot-arena/" target="_blank" rel="noopener" class="source-link">Source: huggingface.co/spaces/lmarena-ai/chatbot-arena</a>
                        <a href="https://arxiv.org/abs/2403.04132" target="_blank" rel="noopener" class="source-link">Methodology: arXiv:2403.04132</a>
                    </div>
                </div>
                <h3>Score Calculation</h3>
                <code class="calc-formula">H = ((Model_Elo - Min_Elo) / (Max_Elo - Min_Elo)) x 100</code>
                <p class="formula-note">Maps the lowest-rated model to 0, highest to 100. Human Preference contributes 20% of TRS.</p>
            </div>
        </section>
        <section class="method-section">
            <h2>4. Knowledge and Comprehension (15%)</h2>
            <div class="method-content">
                <h3>What We Measure</h3>
                <p>Breadth and depth of factual knowledge across academic domains.</p>
                <h3>Primary Data Sources</h3>
                <div class="source-citation">
                    <div class="source-item">
                        <h4>MMLU (Hendrycks et al.)</h4>
                        <p>57 subjects from STEM to humanities, 14,000+ questions.</p>
                        <a href="https://arxiv.org/abs/2009.03300" target="_blank" rel="noopener" class="source-link">Source: arXiv:2009.03300</a>
                    </div>
                    <div class="source-item">
                        <h4>TruthfulQA (Lin et al.)</h4>
                        <p>Tests whether models generate truthful answers.</p>
                        <a href="https://arxiv.org/abs/2109.07958" target="_blank" rel="noopener" class="source-link">Source: arXiv:2109.07958</a>
                    </div>
                </div>
                <h3>Score Calculation</h3>
                <code class="calc-formula">K = (MMLU x 0.70) + (TruthfulQA x 0.30)</code>
            </div>
        </section>
        <section class="method-section">
            <h2>5. Efficiency and Cost (10%)</h2>
            <div class="method-content">
                <h3>What We Measure</h3>
                <p>Performance per dollar.</p>
                <h3>Data Sources</h3>
                <div class="source-citation">
                    <div class="source-item">
                        <h4>API Pricing</h4>
                        <p>Official pricing from OpenAI, Anthropic, Google.</p>
                    </div>
                </div>
                <h3>Score Calculation</h3>
                <code class="calc-formula">E = (Average_Benchmark_Score / Cost_Per_1M_Tokens) x Normalization_Factor</code>
            </div>
        </section>
        <section class="method-section">
            <h2>6. Safety and Reliability (5%)</h2>
            <div class="method-content">
                <h3>What We Measure</h3>
                <p>Consistency, resistance to jailbreaks, refusal of harmful requests.</p>
                <h3>Data Sources</h3>
                <div class="source-citation">
                    <div class="source-item">
                        <h4>Model Provider Safety Reports</h4>
                        <p>Published safety evaluations from providers.</p>
                    </div>
                    <div class="source-item">
                        <h4>Independent Red Team Evaluations</h4>
                        <p>Third-party safety evaluations where available.</p>
                    </div>
                </div>
            </div>
        </section>
        <section class="method-section">
            <h2>Important Limitations</h2>
            <div class="limitations-box">
                <h3>What TRS Does NOT Measure</h3>
                <ul>
                    <li><strong>Future capabilities:</strong> TRS measures current performance only</li>
                    <li><strong>AGI proximity:</strong> We make no claims about AGI</li>
                    <li><strong>Real-world deployment:</strong> Benchmark performance may differ from production</li>
                </ul>
                <h3>Known Biases</h3>
                <ul>
                    <li>English-language bias in most benchmarks</li>
                    <li>Potential training data contamination</li>
                    <li>Self-reported safety data from providers</li>
                </ul>
            </div>
        </section>
        <section class="method-section">
            <h2>Update Frequency</h2>
            <p>TRS scores are updated <strong>weekly</strong>, typically on Mondays.</p>
        </section>
        <section class="method-section">
            <h2>Full Citation List</h2>
            <div class="citations-list">
                <h3>Evaluation Platforms</h3>
                <ul>
                    <li>LMSYS Chatbot Arena: <a href="https://huggingface.co/spaces/lmarena-ai/chatbot-arena/" target="_blank" rel="noopener">huggingface.co/spaces/lmarena-ai/chatbot-arena</a></li>
                    <li>ARC Prize: <a href="https://arcprize.org/" target="_blank" rel="noopener">arcprize.org</a></li>
                    <li>SWE-Bench: <a href="https://www.swebench.com/" target="_blank" rel="noopener">swebench.com</a></li>
                    <li>LiveCodeBench: <a href="https://livecodebench.github.io/" target="_blank" rel="noopener">livecodebench.github.io</a></li>
                    <li>SciCode: <a href="https://scicode-bench.github.io/" target="_blank" rel="noopener">scicode-bench.github.io</a></li>
                </ul>
            </div>
        </section>
    </article>
    <footer>
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <a href="/" class="logo">Training Run</a>
                    <p>Your weekly AI conditioning.</p>
                </div>
                <div class="footer-links">
                    <h4>Resources</h4>
                    <a href="/trsmethodology">TRS Methodology</a>
                    <a href="/scores">Current Scores</a>
                    <a href="/about">About Us</a>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2026 Training Run. All rights reserved.</p>
            </div>
        </div>
    </footer>
</body>
</html>
