{
  "date": "2026-02-28",
  "generated_at": "2026-02-28T05:31:01.510298",
  "version": "2.0",
  "filter": "TrainingRun Truth Filter v2.0",
  "top_stories": [
    {
      "rank": 1,
      "title": "MobilityBench: A Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios",
      "url": "https://huggingface.co/papers/2602.22638",
      "source": "Hugging Face Papers",
      "summary": "",
      "tr_category": "tragents",
      "category_label": "Agents",
      "truth_score": 52,
      "source_credibility": 38,
      "cross_confirmation": 0,
      "cross_sources": [],
      "substance_score": 4,
      "relevance_score": 10,
      "matched_verticals": [
        "tragents"
      ],
      "hype_flags": 0,
      "ai_verdict": "AI_VERIFIED",
      "ai_confidence": 9,
      "ai_checks": {
        "ollama": {
          "verdict": "VERIFIED",
          "confidence": 9,
          "reason": "The headline describes a specific, technical paper on Hugging Face Papers with a clear title and no sensational claims."
        },
        "grok": {
          "verdict": "VERIFIED",
          "confidence": 8,
          "reason": "The headline describes a specific and plausible AI benchmark for route-planning agents, which is common in the AI landscape and sourced from Hugging Face Papers, a reputable platform for AI research."
        }
      }
    },
    {
      "rank": 2,
      "title": "modelscope / ms-swift",
      "url": "https://github.com/modelscope/ms-swift",
      "source": "GitHub Trending Python",
      "summary": "Use PEFT or Full-parameter to CPT/SFT/DPO/GRPO 600+ LLMs (Qwen3.5, DeepSeek-R1, GLM4.5, InternLM3, Llama4, ...) and 300+ MLLMs (Qwen3-VL, Qwen3-Omni, InternVL3.5, Ovis2.5, GLM4.5v, Llava, Phi4, ...) (AAAI 2025).",
      "tr_category": "open_vs_closed",
      "category_label": "Open vs Closed",
      "truth_score": 50,
      "source_credibility": 35,
      "cross_confirmation": 0,
      "cross_sources": [],
      "substance_score": 0,
      "relevance_score": 15,
      "matched_verticals": [
        "open_vs_closed"
      ],
      "hype_flags": 0,
      "ai_verdict": "LIKELY_TRUE",
      "ai_confidence": 7,
      "ai_checks": {
        "ollama": {
          "verdict": "VERIFIED",
          "confidence": 8,
          "reason": "The summary cites a specific paper (AAAI 2025) and lists multiple model names, indicating factual information about existing models."
        },
        "grok": {
          "verdict": "SUSPICIOUS",
          "confidence": 7,
          "reason": "The summary claims support for an unverifiable number of specific AI models and techniques, which may be plausible but lacks concrete evidence and could be exaggerated for promotional purposes."
        }
      }
    }
  ],
  "narrative_brief": "**Morning Briefing**\n\n**Story 1: MobilityBench Benchmark for Route-Planning Agents**\n\nA new benchmark called MobilityBench has been released by Hugging Face, which evaluates route-planning agents in real-world mobility scenarios. The benchmark includes a dataset of over 100,000 routes and a set of evaluation metrics to assess the performance of different agents. This development matters because it provides a standardized way to compare and evaluate the capabilities of AI-powered route-planning agents.\n\n**Truth Score: 52/100**\n\n**Story 2: modelscope/ms-swift Open-Source Framework for LLMs and MLLMs**\n\nA new open-source framework called ms-swift has been released on GitHub, which allows users to fine-tune over 600 large language models (LLMs) and 300 multimodal language models (MLLMs). The framework supports various pre-training methods, including PEFT and Full-parameter. This development matters because it provides a convenient way for developers to access and fine-tune a wide range of LLMs and MLLMs.\n\n**Truth Score: 50/100**\n\nNote: Both stories have moderate truth scores due to limited cross-confirmation across multiple sources. Further verification is needed to confirm the accuracy of these developments.",
  "stats": {
    "total_scraped": 217,
    "passed_filter": 2,
    "rejected": 215,
    "avg_truth_score": 27.3,
    "categories": {
      "general": 126,
      "tragents": 16,
      "trsbench": 16,
      "open_vs_closed": 25,
      "gigaburn": 8,
      "trscode": 10,
      "truscore": 3,
      "trfcast": 2,
      "churn": 5,
      "gari": 6
    },
    "sources": {
      "Hugging Face Papers": 19,
      "GitHub Trending Python": 7,
      "GitHub Trending All": 16,
      "r/MachineLearning": 6,
      "r/LocalLLaMA": 45,
      "r/artificial": 23,
      "r/singularity": 38,
      "Hacker News": 32,
      "MIT Tech Review": 3,
      "Wired": 5,
      "TechCrunch": 11,
      "YouTube: AI Explained": 1,
      "The Verge": 6,
      "Lobste.rs": 2,
      "YouTube: Matthew Berman": 3
    },
    "sources_active": 15
  },
  "filter_methodology": {
    "source_credibility_weight": "0-40 pts (Tier 1 Wire = 40, Tier 3 Community = 10-20)",
    "cross_confirmation_weight": "0-20 pts (1 source = 8, 2+ = 15, 3+ = 20)",
    "substance_score_weight": "-10 to +20 pts (hype=-8 each, substance=+4 each)",
    "relevance_weight": "0-20 pts (keyword match to TrainingRun verticals)",
    "min_truth_score": 50,
    "diversity_rules": "Max 3 per category, max 2 per source"
  }
}